{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Ungraded Lab: Data Labeling\n",
    "\n",
    "----------------\n",
    "\n",
    "Welcome to the ungraded lab for Week 3 of Machine Learning Engineering in Production. In this lab, you will see how the data labeling process affects the performance of a classification model. Labeling data is usually a labor-intensive and costly task, but the model's performance relies on high quality labels.\n",
    "\n",
    "As you saw in the lectures, there are strategies you can use for labeling data. In the example with the iguanas, you saw three valid labeling strategies that follow different criteria:\n",
    "\n",
    "<table><tr><td><img src='assets/iguanas1.png'></td><td><img src='assets/iguanas2.png'></td><td><img src='assets/iguanas3.png'></td></tr></table>\n",
    "\n",
    "**Each labeling strategy is the result of labelers following different rules**. Employing labelers who use different criteria will negatively impact your learning algorithm. Rather, your dataset should have consistent labeling throughout.\n",
    "\n",
    "In this lab, you will explore how different strategies affect the performance of a machine learning model by simulating the process of having different labelers label the data. You will achieve this effect by performing automatic labeling based on a defined set of rules.\n",
    "\n",
    "**Your main objective is to understand how labeling affects the performance of machine learning models by comparing performance across labeling options**, including:\n",
    "1. Randomly generated labels (performance lower bound)\n",
    "2. Automatic generated labels based on three different labeling strategies\n",
    "3. True labels (performance upper bound)\n",
    "\n",
    "Labeling concepts apply to all types of supervised learning, not just computer vision tasks. In this lab, you will work with text data: a dataset containing comments from the 2015 top 5 most popular Youtube videos. Each comment can be classified as `spam` or `not_spam` depending on its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "Begin by cloning the repo that hosts the dataset (not needed if running the notebook on Coursera):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you're running the notebook outside the Coursera environment\n",
    "# !git clone https://github.com/https-deeplearning-ai/MLEP-public.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 5 CSV files, one for each video. You'll use a pandas `DataFrame` to load in the data from the CSV. The following helper function will load the data using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labeled_spam_dataset():\n",
    "    \"\"\"Load labeled spam dataset.\"\"\"\n",
    "\n",
    "    # Path where csv files are located\n",
    "    base_path = \"./data\"\n",
    "\n",
    "    # List of csv files with full path\n",
    "    csv_files = [os.path.join(base_path, csv) for csv in os.listdir(base_path)]\n",
    "\n",
    "    # List of dataframes for each file\n",
    "    dfs = [pd.read_csv(filename) for filename in csv_files]\n",
    "\n",
    "    # Concatenate dataframes into a single one\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={\"CONTENT\": \"text\", \"CLASS\": \"label\"})\n",
    "\n",
    "    # Set a seed for the order of rows\n",
    "    df = df.sample(frac=1, random_state=423)\n",
    "    \n",
    "    return df.reset_index()\n",
    "\n",
    "\n",
    "# Save the dataframe into the df_labeled variable\n",
    "df_labeled = load_labeled_spam_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the top 5 rows of the `DataFrame` to better understand the dataset's organization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the first 5 rows\n",
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further inspection and preprocessing\n",
    "\n",
    "\n",
    "### Checking for data imbalance\n",
    "\n",
    "You may naturally assume that the data you are working on is balanced, meaning that it contains a similar proportion of examples across all classes. Before moving forward, you will test this assumption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print actual value count\n",
    "print(f\"Value counts for each class:\\n\\n{df_labeled.label.value_counts()}\\n\")\n",
    "\n",
    "# Display pie chart to visually check the proportion\n",
    "df_labeled.label.value_counts().plot.pie(y='label', title='Proportion of each class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains roughly the same number of data points for each class, so class imbalance is not an issue you will need to address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the dataset\n",
    "\n",
    "When you inspected the dataset initially, you may have noticed that the `DataFrame` includes information irrelevant to your task. Since you are only interested in the comments, their labels, and the video they belong to, you can drop the remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns\n",
    "df_labeled = df_labeled.drop(['index', 'COMMENT_ID', 'AUTHOR', 'DATE'], axis=1)\n",
    "\n",
    "# Look at the cleaned dataset\n",
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset only includes the information you are going to use moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "\n",
    "Since you will want to later measure the performance of the different models you train, it's a good idea to split the data into train and test sets. As a safety measure, use stratification to maintain the proportion of examples of each class in your split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Save the text into the X variable\n",
    "X = df_labeled.drop(\"label\", axis=1)\n",
    "\n",
    "# Save the true labels into the y variable\n",
    "y = df_labeled[\"label\"]\n",
    "\n",
    "# Use 1/5 of the data for testing later\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Print number of comments for each set\n",
    "print(f\"There are {X_train.shape[0]} comments for training.\")\n",
    "print(f\"There are {X_test.shape[0]} comments for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to double check that the stratification worked as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "y_train.value_counts().plot.pie(y='label', title='Proportion of each class for train set', figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "y_test.value_counts().plot.pie(y='label', title='Proportion of each class for test set', figsize=(10, 6))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the train and test sets include a balanced proportion of examples per class. Great! You successfully implemented stratification.\n",
    "\n",
    "Now you'll start with the data labeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Labeling \n",
    "\n",
    "### Establishing performance lower and upper bounds for reference\n",
    "\n",
    "To properly compare different labeling strategies, you need to establish a baseline for model accuracy. In this case, you will establish both a lower and an upper bound to compare different strategies against. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy of a labeling strategy\n",
    "\n",
    "ðŸ”—[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) is a handy tool included in the `sklearn` ecosystem to encode text-based data.\n",
    "\n",
    "For more information on working with text data using `sklearn`, check out this ðŸ”—[tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Allow unigrams and bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the text encoding is defined, you need to select a model to make predictions. For simplicity, you will use a ðŸ”—[Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) classifier. This model is well suited for text classification and is fairly quick to train.\n",
    "\n",
    "Now you'll define a function to handle the model fitting and print out the accuracy on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def calculate_accuracy(X_tr, y_tr, X_te=X_test, y_te=y_test, \n",
    "                       clf=MultinomialNB(), vectorizer=vectorizer):\n",
    "    \n",
    "    # Encode train text\n",
    "    X_train_vect = vectorizer.fit_transform(X_tr.text.tolist())\n",
    "    \n",
    "    # Fit model\n",
    "    clf.fit(X=X_train_vect, y=y_tr)\n",
    "    \n",
    "    # Vectorize test text\n",
    "    X_test_vect = vectorizer.transform(X_te.text.tolist())\n",
    "    \n",
    "    # Make predictions for the test set\n",
    "    preds = clf.predict(X_test_vect)\n",
    "    \n",
    "    # Return accuracy score\n",
    "    return accuracy_score(preds, y_te)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a dictionary to store the accuracy of each labeling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dictionary\n",
    "accs = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Labeling\n",
    "\n",
    "Generating random labels is a useful technique for establishing a lower bound, since you should expect any viable labeling strategy to outperform random chance. \n",
    "\n",
    "Now, calculate the accuracy for the random labeling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate random labels\n",
    "rnd_labels = np.random.randint(0, 2, X_train.shape[0])\n",
    "\n",
    "# Feed them alongside X_train to calculate_accuracy function\n",
    "rnd_acc = calculate_accuracy(X_train, rnd_labels)\n",
    "\n",
    "rnd_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see a different accuracy every time you run the previous cell because the labels are determined randomly. Since this is a binary classification problem and both classes are balanced, you can expect to see accuracies around 50%.\n",
    "\n",
    "Calculate the average accuracy over 10 trials to get a better sense of the base rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to save accuracies\n",
    "rnd_accs = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Add every accuracy to the list\n",
    "    rnd_accs.append(calculate_accuracy(X_train, np.random.randint(0, 2, X_train.shape[0])))\n",
    "\n",
    "# Save result in accs dictionary\n",
    "accs['random-labels'] = sum(rnd_accs)/len(rnd_accs)\n",
    "\n",
    "# Print result\n",
    "print(f\"The random labeling method achieved an accuracy of {accs['random-labels']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since random labeling completely disregards the information in the solution space and just guesses the correct label, it's difficult to achieve worse accuracy using other strategies. This lower bound serves as reference when comparing other labeling methods. If a certain labeling method doesn't outperform your lower bound, it can be disregarded as a viable option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling with true values\n",
    "\n",
    "Now you'll look at the other end of the spectrum: using the ground truth labels. Retrain your classifier with the actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy when using the true labels\n",
    "true_acc = calculate_accuracy(X_train, y_train)\n",
    "\n",
    "# Save the result\n",
    "accs['true-labels'] = true_acc\n",
    "\n",
    "print(f\"The true labeling method achieved and accuracy of {accs['true-labels']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with the true labels produced a noticeable boost in accuracy. This result is expected, as the classifier can now properly identify patterns in the training data which were lacking with randomly generated labels. \n",
    "\n",
    "You could achieve higher accuracy by fine-tunning the model or even selecting a different one. For the time being, however, you will keep the model as-is. When trying out different automatic labeling algorithms in the next sections, you'll strive for this ground truth accuracy while holding other aspects of training constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic labeling - Trying out different labeling strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you don't have access to the true labels for this dataset -- a common situation in machine learning. It's natural to assume that there are patterns in the data that can provide clues about the correct labels. These patterns are dependent on the kind of data you are working with; to even hypothesize which patterns exist usually requires great domain knowledge.\n",
    "\n",
    "For this particular dataset, you can likely come up with some useful rules that might help distinguish a spam YouTube comment from non-spam. In the following section, you will perform automatic labeling using such rules. **Think of each iteration of this process as a labeler with different criteria for labeling**, and your job is to hire the most promising one.\n",
    "\n",
    "Notice the word **rules**. To perform automatic labeling, you will define some rules such as \"if the comment contains the word 'free', then classify it as spam\".\n",
    "\n",
    "First things first: define how you are going to encode the labeling:\n",
    "- `SPAM` is represented by 1\n",
    "\n",
    "\n",
    "- `NOT_SPAM` by 0 \n",
    "\n",
    "\n",
    "- `NO_LABEL` by -1\n",
    "\n",
    "\n",
    "You will use the `NO_LABEL` keyword when the rules you come up with don't apply to some data points. In those cases, it is better to avoid giving a label rather than guessing, which you already saw yields poor results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First iteration - Define some rules\n",
    "\n",
    "For this first iteration, you will create three rules based on your intuition about common patterns that appear in spam comments. The rules are simple: classify as SPAM if any of the following patterns is present within the comment, or NO_LABEL otherwise:\n",
    "- `free` - spam comments usually lure users by promoting free stuff\n",
    "- `subs` - spam comments tend to ask users to subscribe to some website or channel\n",
    "- `http` - spam comments include links very frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_rules_1(x):\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    x = x.lower()\n",
    "    \n",
    "    # Define list of rules\n",
    "    rules = [\n",
    "        \"free\" in x,\n",
    "        \"subs\" in x,\n",
    "        \"http\" in x\n",
    "    ]\n",
    "    \n",
    "    # If the comment falls under any of the rules classify as SPAM\n",
    "    if any(rules):\n",
    "        return 1\n",
    "    \n",
    "    # Otherwise, NO_LABEL\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the rules to the comments in the train set\n",
    "labels = [labeling_rules_1(label) for label in X_train.text]\n",
    "\n",
    "# Convert to a numpy array\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "# Take a look at the automatic labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lots of points, the automatic labeling algorithm decided to not settle for a label, which is expected since this rule only identifies spam comments and can't identify non-spam ones. These points should be deleted, since they don't provide information about the classification process and tend to hurt performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the automatic labeled version of X_train by removing points with NO_LABEL label\n",
    "X_train_al = X_train[labels != -1]\n",
    "\n",
    "# Remove predictions with NO_LABEL label\n",
    "labels_al = labels[labels != -1]\n",
    "\n",
    "print(f\"Predictions with concrete label have shape: {labels_al.shape}\")\n",
    "\n",
    "print(f\"Proportion of data points kept: {labels_al.shape[0]/labels.shape[0]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that only 375 data points remained out of the original 1564. The rules defined didn't provide enough context for the labeling algorithm to settle on a label, so around 75% of the data has been trimmed.\n",
    "\n",
    "Now you'll test the accuracy of the model when using these automatically generated labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy when using these labels\n",
    "iter_1_acc = calculate_accuracy(X_train_al, labels_al)\n",
    "\n",
    "# Display accuracy\n",
    "print(f\"First iteration of automatic labeling has an accuracy of {iter_1_acc*100:.2f}%\")\n",
    "\n",
    "# Save the result\n",
    "accs['first-iteration'] = iter_1_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this accuracy to the baselines by plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(accs=accs):\n",
    "    colors = list(\"rgbcmy\")\n",
    "    items_num = len(accs)\n",
    "    cont = 1\n",
    "\n",
    "    for x, y in accs.items():\n",
    "        if x in ['true-labels', 'random-labels', 'true-labels-best-clf']:\n",
    "            plt.hlines(y, 0, (items_num-2)*2, colors=colors.pop())\n",
    "        else:\n",
    "            plt.scatter(cont, y, s=100)\n",
    "            cont+=2\n",
    "    plt.legend(accs.keys(), loc=\"center left\",bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "plot_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first iteration had an accuracy very close to the random labeling, which you should strive to outperform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving forward, define the `label_given_rules` function that performs all of the steps you just saw: \n",
    "- Apply the rules to a dataframe of comments\n",
    "- Cast the resulting labels to a numpy array\n",
    "- Delete all data points with NO_LABEL as label\n",
    "- Calculate the accuracy of the model using the automatic labels\n",
    "- Save the accuracy for plotting\n",
    "- Print some useful metrics of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_given_rules(df, rules_function, name, \n",
    "                      accs_dict=accs, verbose=True):\n",
    "    \n",
    "    # Apply labeling rules to the comments\n",
    "    labels = [rules_function(label) for label in df.text]\n",
    "    \n",
    "    # Convert to a numpy array\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    # Save initial number of data points\n",
    "    initial_size = labels.shape[0]\n",
    "    \n",
    "    # Trim points with NO_LABEL label\n",
    "    X_train_al = df[labels != -1]\n",
    "    labels = labels[labels != -1]\n",
    "    \n",
    "    # Save number of data points after trimming\n",
    "    final_size = labels.shape[0]\n",
    "    \n",
    "    # Compute accuracy\n",
    "    acc = calculate_accuracy(X_train_al, labels)\n",
    "    \n",
    "    # Print useful information\n",
    "    if verbose:\n",
    "        print(f\"Proportion of data points kept: {final_size/initial_size*100:.2f}%\\n\")\n",
    "        print(f\"{name} labeling has an accuracy of {acc*100:.2f}%\\n\")\n",
    "        \n",
    "    # Save accuracy to accuracies dictionary\n",
    "    accs_dict[name] = acc\n",
    "    \n",
    "    return X_train_al, labels, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, you will come up with rules that have better coverage of the training data, which makes pattern discovery an easier task. Since the rules were only able to label as either SPAM or NO_LABEL, you will also create some rules that help identify NOT_SPAM comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second iteration - Coming up with better rules\n",
    "\n",
    "Inspecting the comments in the dataset more closely, you can distinguish more patterns. For example, NOT_SPAM comments often  references either the number of views (since these were the most watched videos of 2015) or the song in the video and its contents. As for spam comments, other common patterns are to promote gifts or ask to follow some channel or website.\n",
    "\n",
    "Now, you'll create some new rules that include these patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_rules_2(x):\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    x = x.lower()\n",
    "    \n",
    "    # Define list of rules to classify as NOT_SPAM\n",
    "    not_spam_rules = [\n",
    "        \"view\" in x,\n",
    "        \"song\" in x\n",
    "    ]\n",
    "    \n",
    "    # Define list of rules to classify as SPAM\n",
    "    spam_rules = [\n",
    "        \"free\" in x,\n",
    "        \"subs\" in x,\n",
    "        \"gift\" in x,\n",
    "        \"follow\" in x,\n",
    "        \"http\" in x\n",
    "    ]\n",
    "    \n",
    "    # Classify depending on the rules\n",
    "    if any(not_spam_rules):\n",
    "        return 0\n",
    "    \n",
    "    if any(spam_rules):\n",
    "        return 1\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new set of rules looks more promising, as it includes more patterns to classify as SPAM as well as some patterns to classify as NOT_SPAM. These improved rules should result in more data points with a label (as opposed to NO_LABEL).\n",
    "\n",
    "Check if this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_given_rules(X_train, labeling_rules_2, \"second-iteration\")\n",
    "\n",
    "plot_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time 44% of the original dataset was given a decisive label and there were data points for both labels. This result helped the model reach a higher accuracy compared with the first iteration. Its accuracy is now considerably higher than random labeling, but it is still very far from the upper bound.\n",
    "\n",
    "Now you'll see if you can make it even better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Iteration - Even more rules\n",
    "\n",
    "The rules you have defined so far are doing a fair job. Now, you'll add two additional rules: one each for classifying SPAM and NOT_SPAM comments.\n",
    "\n",
    "At a glance, it looks like NOT_SPAM comments are usually shorter, possibly because they don't include hyperlinks or because in general they tend to be more concrete (such as \"I love this song!\").\n",
    "\n",
    "Take a look at the average number of characters for SPAM comments vs NOT_SPAM ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "print(f\"NOT_SPAM comments have an average of {mean([len(t) for t in df_labeled[df_labeled.label==0].text]):.2f} characters.\")\n",
    "print(f\"SPAM comments have an average of {mean([len(t) for t in df_labeled[df_labeled.label==1].text]):.2f} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sure looks like there is a big difference in the number of characters for both types of comments.\n",
    "\n",
    "To decide on a threshold to classify as NOT_SPAM, you can plot a histogram of the number of characters in NOT_SPAM comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(t) for t in df_labeled[df_labeled.label==0].text], range=(0,100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of NOT_SPAM comments have 30 characters or fewer, so you'll use 30 as your threshold.\n",
    "\n",
    "Another prevalent pattern in spam comments is to ask users to \"check out\" a channel, website, or link.\n",
    "\n",
    "Add these two new rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_rules_3(x):\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    x = x.lower()\n",
    "    \n",
    "    # Define list of rules to classify as NOT_SPAM\n",
    "    not_spam_rules = [\n",
    "        \"view\" in x,\n",
    "        \"song\" in x,\n",
    "        len(x) < 30\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Define list of rules to classify as SPAM\n",
    "    spam_rules = [\n",
    "        \"free\" in x,\n",
    "        \"subs\" in x,\n",
    "        \"gift\" in x,\n",
    "        \"follow\" in x,\n",
    "        \"http\" in x,\n",
    "        \"check out\" in x\n",
    "    ]\n",
    "    \n",
    "    # Classify depending on the rules\n",
    "    if any(not_spam_rules):\n",
    "        return 0\n",
    "    \n",
    "    if any(spam_rules):\n",
    "        return 1\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_given_rules(X_train, labeling_rules_3, \"third-iteration\")\n",
    "\n",
    "plot_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These new rules do a pretty good job at both SPAM and NOT_SPAM, with good coverage of the dataset and good model accuracy. This labeling strategy reached an accuracy of ~88%! You are getting closer and closer to the upper bound defined by the true labels.\n",
    "\n",
    "If you're curious, add some more rules to improve accuracy and try them out yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Come up with your own rules\n",
    "\n",
    "The following cells contain some code to help you inspect the dataset for patterns and to test out these patterns. The ones used before are commented out in case you want start from scratch or re-use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pandas to print out all rows to check the complete dataset\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Check NOT_SPAM comments\n",
    "df_labeled[df_labeled.label==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SPAM comments\n",
    "df_labeled[df_labeled.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_labeling_rules(x):\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    x = x.lower()\n",
    "    \n",
    "    # Define your rules for classifying as NOT_SPAM\n",
    "    not_spam_rules = [\n",
    "#         \"view\" in x,\n",
    "#         \"song\" in x,\n",
    "#         len(x) < 30\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Define your rules for classifying as SPAM\n",
    "    spam_rules = [\n",
    "#         \"free\" in x,\n",
    "#         \"subs\" in x,\n",
    "#         \"gift\" in x,\n",
    "#         \"follow\" in x,\n",
    "#         \"http\" in x,\n",
    "#         \"check out\" in x\n",
    "    ]\n",
    "    \n",
    "    # Classify depending on your rules\n",
    "    if any(not_spam_rules):\n",
    "        return 0\n",
    "    \n",
    "    if any(spam_rules):\n",
    "        return 1\n",
    "    \n",
    "    return -1\n",
    "\n",
    "\n",
    "try:\n",
    "    label_given_rules(X_train, your_labeling_rules, \"your-iteration\")\n",
    "    plot_accuracies()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"You have not defined any rules.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this ungraded lab!**\n",
    "\n",
    "By now, you should have a better understanding of the importance of properly labeled data. In general, **the better your labels are, the better your models will be**. The process of correctly labeling data is a very complex one. **Remember, you can think of each one of the iterations of the automatic labeling process to be a different labeler with different criteria for labeling**. If you assume you are hiring labelers, you will want to hire the last one for sure! \n",
    "\n",
    "Also keep in mind that establishing baselines will help you evaluate how well your models are performing.\n",
    "\n",
    "**Keep it up!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C1W3_Data_Labeling_Ungraded_Lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
